{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二章 张量\n",
    "\n",
    "　　Tensor，中文叫张量，逻辑上是一个多维数组，类似NumPy的ndarrays，０维对应标量，１维对应向量，２维对应矩阵，其优势在于：\n",
    "  \n",
    " 　　**１．并行加速**：　Tensor实现了许多并行算法，可用多核CPU和CUDA加速。众所周知，编写高效并行算法，极具挑战，Tensor简化了这项工作。特别地，Tensor建立在ATen库上，源码用C/C++和CUDA实现，效率有保证。\n",
    " \n",
    "　　**２．自动微分：**　深度学习算法的基础是反向传播求导，从0.4版开始，Tensor可设置requries_grad来支持自动微分。\n",
    "    \n",
    "　　**３．数据共享**：　Tensor含头部信息和数据存储区。头部保存形状(size)、步长(stride)、数据类型(type)等，数据则保存在一块尽可能连续的内存(或显存)中。一般头部占内存少，数据占内存大，所以相关数据会尽可能共享存储，是否共享，可用函数`id()`验证。\n",
    "  \n",
    "　　本章尽量用实例来讲解Tensor([全面文档请参阅](https://pytorch.org/docs/master/torch.html))。开始之间，让我们导入最重要的模块torch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Version:  1.0.0a0+17c6d16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch Version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 表格排序\n",
    "   Torch支持排序，函数原型如下：\n",
    "   \n",
    "   sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor)\n",
    "   \n",
    "   能对输入的张量按给定的维度排序。下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6674, 0.6190, 0.7941, 0.9073],\n",
      "        [0.9862, 0.3700, 0.6737, 0.3410],\n",
      "        [0.7595, 0.7885, 0.9475, 0.3307]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6674, 0.3700, 0.6737, 0.3307],\n",
       "         [0.7595, 0.6190, 0.7941, 0.3410],\n",
       "         [0.9862, 0.7885, 0.9475, 0.9073]]), tensor([[0, 1, 1, 2],\n",
       "         [2, 0, 0, 1],\n",
       "         [1, 2, 2, 0]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3,4)\n",
    "print(x)\n",
    "torch.sort(x, dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　上例每列从小到大，排序正确，但它**打乱了每行的联系**，在大多数情况下，我们希望保持行的一致性，例如只按第一列排序，步骤如下：\n",
    "  \n",
    "　　1. 选取第一列排序，得到排序索引；\n",
    "  \n",
    "　　2. 根据排序索引，按行挑选数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Data:\n",
      "tensor([[0.2727, 0.6275, 0.5062, 0.4976, 0.3659],\n",
      "        [0.7815, 0.5055, 0.5457, 0.9314, 0.7082],\n",
      "        [0.5755, 0.6025, 0.9368, 0.1088, 0.7734]])\n",
      "Sorted Data:\n",
      "tensor([[0.2727, 0.6275, 0.5062, 0.4976, 0.3659],\n",
      "        [0.5755, 0.6025, 0.9368, 0.1088, 0.7734],\n",
      "        [0.7815, 0.5055, 0.5457, 0.9314, 0.7082]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sort_table(table, dim, no):\n",
    "    \"\"\"sort_table(table, dim, sort_index) --> Tensor .\"\"\"\n",
    "\n",
    "    assert(table.dim() == 2)\n",
    "    \n",
    "    # narrow(input, dimension, start, length) -> Tensor\n",
    "    n = table.narrow((dim + 1) % 2, no, 1).view(-1)\n",
    "\n",
    "    # sort(input, dim=None, descending=False, out=None) -> (Tensor, LongTensor)\n",
    "    _, index = n.sort()\n",
    "\n",
    "    # index_select(input, dim, index, out=None) -> Tensor\n",
    "    r = torch.index_select(table, dim, index)\n",
    "\n",
    "    return r\n",
    "\n",
    "def test_sort_table():\n",
    "    x = torch.rand(3, 5)\n",
    "    r = sort_table(x, 0, 0)\n",
    "    print(\"Random Data:\")\n",
    "    print(x)\n",
    "    print(\"Sorted Data:\")\n",
    "    print(r)\n",
    "\n",
    "test_sort_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 图像积分\n",
    "\n",
    "　　积分图像在Viola的人脸实时识别中发挥了巨大威力，它是一张图像，图像某点的颜色定义为：原始图像原点到该点矩形区内的各点颜色和。通过积分图像，原始图像任意矩形区的颜色之和就可以通过**“加减”**操作在**常数时间**内完成，它是快速Box滤波算法的关键。本质上，它就是一个**累加矩阵**，因此，我们可以使用Tensor实现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19.],\n",
      "        [20., 21., 22., 23., 24.]])\n",
      "tensor([[  0.,   1.,   3.,   6.,  10.],\n",
      "        [  5.,  12.,  21.,  32.,  45.],\n",
      "        [ 15.,  33.,  54.,  78., 105.],\n",
      "        [ 30.,  64., 102., 144., 190.],\n",
      "        [ 50., 105., 165., 230., 300.]])\n"
     ]
    }
   ],
   "source": [
    "def integrate_image():\n",
    "    a = torch.arange(25).float().view(5, 5)\n",
    "    print(a)\n",
    "    b = a.cumsum(dim=0).cumsum(dim=1)\n",
    "    print(b)\n",
    "\n",
    "integrate_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**常见累加求和的并行算法是归并(Reduce)操作，在并行算法中十分常见。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 线性回归\n",
    "\n",
    "　　**数学原理**\n",
    "\n",
    "　　给定数据集$D = \\{ (x_{1}, y_{1}), (x_{2}, y_{2}, ..., (x_{m}, y_{m})\\}$，线性回归希望找到函数f(x)，满足$f(x_{i}) = wx_{i} + b$而且$f(x_{i})$能够和$y_{i}$尽可能接近。\n",
    "\n",
    "　　如何才能学到参数w和b呢？需要确定如何衡量f(x)与y之差，一般通过损失函数（Loss Funciton)来衡量：\n",
    "\n",
    "　　　　$Loss = \\sum_{i=1}^{m}(f(x_{i})-y_{i})^2$。\n",
    "\n",
    "　　这就是著名的**均方误差**。我们要做的就是找到$w^{*}$和$b^{*}$，使得：\n",
    "\n",
    "　　　　$(w^{*}, b^{*}) = arg min_{w},_{b}\\sum_{i=1}^{m}(f(x_{i})-y_{i})^2$\n",
    "\n",
    "　　　　　　　　$ = arg min_{w},_{b}\\sum_{i=1}^{m}(y_{i}-wx_{i}-b)^2$\n",
    "\n",
    "　　均方误差直观，有好的几何意义，对应欧式距离。现在要求解损失函数的最小值，方法是求它的偏导数，让偏导等于0来估计参数，即：\n",
    "\n",
    "　　　　$\\frac{\\partial Loss_{(w, b)}}{\\partial w} = 2(w\\sum_{i=1}^{m}x_{i}^{2}-\\sum_{i=1}^{m}(y_{i}-b)x_{i})=0$\n",
    "\n",
    "　　　　$\\frac{\\partial Loss_{(w, b)}}{\\partial b} = 2(mb-\\sum_{i=1}^{m}(y_{i}-wx_{i}))=0$\n",
    "\n",
    "　　求解以上两式，就可以得到最优解。幸运的是，对线性函数，我们有最优解，不幸的是，对大多数非线性函数，只有数值近似解，下面给出用Torch求解的例子。\n",
    "  \n",
    "  　　**实际例子**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average_loss loss: 0.017373\n",
      "Epoch 1, average_loss loss: 0.003713\n",
      "Epoch 2, average_loss loss: 0.001729\n",
      "Epoch 3, average_loss loss: 0.001107\n",
      "Epoch 4, average_loss loss: 0.000770\n",
      "Epoch 5, average_loss loss: 0.000541\n",
      "Epoch 6, average_loss loss: 0.000379\n",
      "Epoch 7, average_loss loss: 0.000267\n",
      "Epoch 8, average_loss loss: 0.000188\n",
      "Epoch 9, average_loss loss: 0.000132\n",
      "Epoch 10, average_loss loss: 0.000093\n",
      "Loss: 0.001778 after 10 epoch\n",
      "==> Learned function:\ty = +0.9773x^2 -0.8348x^1 +0.1609\n",
      "==> Actual  function:\ty = +0.9508x^2 -0.8457x^1 +0.2215\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "num_inputs = 2\n",
    "num_outpus = 1\n",
    "num_examples = 512\n",
    "\n",
    "\n",
    "true_w = torch.randn(num_inputs, num_outpus)\n",
    "true_b = torch.randn(num_outpus)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"f(x) = X*w + b == > Y\"\"\"\n",
    "    return x.mm(true_w) + true_b.item()\n",
    "\n",
    "\n",
    "def make_feat(x):\n",
    "    \"\"\"Builds a matrix with columns [x^4, x^3, x^2, x^1].\"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    return torch.cat([x ** i for i in range(num_inputs, 0, -1)], 1)\n",
    "\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.4f}x^{} '.format(w, len(W) - i)\n",
    "    result += '{:+.4f}'.format(b[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "def make_data(nums):\n",
    "    \"\"\"\n",
    "    # x = torch.randn(num_examples, num_inputs)\n",
    "    # y = torch.randn(num_examples, num_outpus)\n",
    "    \"\"\"\n",
    "    x = make_feat(torch.randn(nums))\n",
    "    y = f(x)\n",
    "    return x, y\n",
    "\n",
    "# 1.创建数据\n",
    "x, y = make_data(num_examples)\n",
    "dataset = TensorDataset(x, y)\n",
    "trainloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2.定义模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(num_inputs, num_outpus)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# 3. 损失函数\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4. 优化方法\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-2)\n",
    "\n",
    "# 5. 训练模型\n",
    "net.train()\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in trainloader:\n",
    "        x, y = data\n",
    "\n",
    "        # # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "    average_loss = total_loss/num_examples\n",
    "    print(\"Epoch %d, average_loss loss: %f\" %(epoch, average_loss))\n",
    "    if average_loss < 0.0001:\n",
    "        break\n",
    "\n",
    "# 6. 验证模型\n",
    "print('Loss: {:.6f} after {} epoch'.format(loss, epoch))\n",
    "print('==> Learned function:\\t' + poly_desc(net.fc.weight.view(-1), net.fc.bias))\n",
    "print('==> Actual  function:\\t' + poly_desc(true_w.view(-1), true_b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　这个例子非常简单，但也遵循深度学习的基本套路：创建数据，定义模型，选定损失函数和优化方法，训练模型，验证模型。\n",
    "  \n",
    "　　注意由**num_inputs**控制多项式的次数，把这个多项式看成核函数，就是机器学习中的核方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 图像滤波\n",
    "　　我们知道，二维卷积就是滤波，一个自然的问题是：**Torch可以代替的传统软件来对图像滤波吗？**\n",
    "  \n",
    "　　答案是：**能！**而且代码简洁，运行高效。\n",
    "\n",
    "　　首先要解决的问题是：图像的读取和图像与Tensor之间的互相转换。\n",
    "  \n",
    "　　1. 安装视觉处理包torchvision，我们使用的PyTorch是最新版，建议从源码安装，可以避免烦人的包依赖，地址在：https://github.com/pytorch/vision；\n",
    "      \n",
    "　　2. 安装图像处理包Pillow，anaconda默认已经安装，如果没有安装，请安装。\n",
    "      \n",
    "　　注意**数据格式:** 原始PIL图像格式呈现为HxWxC，取值[0,255]，H，W，C分别代表图像高，宽和颜色通道数量；Torch网络接受的输入数据格式为BxCxHxW，取值[0,1.0]，B代表批大小(Batch_Size)。下面给出读取图像和数据转换的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def open(filename):\n",
    "    return Image.open(filename).convert('RGB')\n",
    "\n",
    "\n",
    "def to_tensor(image):\n",
    "    \"\"\"\n",
    "    return 1xCxHxW tensor\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    t = transform(image)\n",
    "    return t.unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def from_tensor(tensor):\n",
    "    \"\"\"\n",
    "    tensor format: 1xCxHxW\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToPILImage()])\n",
    "    return transform(tensor.squeeze(0).cpu())\n",
    "\n",
    "img = open(\"images/roma.jpg\")\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/roma.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.1 高斯滤波\n",
    "\n",
    "　　下面选用3x3高斯核，注意**卷积分组(即groups=3)**的设定，为了更容易看出滤波效果，滤波进行了１０次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 ms ± 83.2 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "class GaussFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    3x3 Guassian filter\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GaussFilter, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            3, 3, kernel_size=3, padding=1, groups=3, bias=False)\n",
    "\n",
    "        # self.conv.bias.data.fill_(0.0)\n",
    "        self.conv.weight.data.fill_(0.0625)\n",
    "        self.conv.weight.data[:, :, 0, 1] = 0.125\n",
    "        self.conv.weight.data[:, :, 1, 0] = 0.125\n",
    "        self.conv.weight.data[:, :, 1, 2] = 0.125\n",
    "        self.conv.weight.data[:, :, 2, 1] = 0.125\n",
    "        self.conv.weight.data[:, :, 1, 1] = 0.25\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "def gauss_filter(device, img):\n",
    "    model = GaussFilter()\n",
    "    model = model.to(device)\n",
    "    t = to_tensor(img)\n",
    "    for i in range(10):\n",
    "        t = model(t)\n",
    "        t.detach_()\n",
    "\n",
    "    return from_tensor(t)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    img = open(\"images/roma.jpg\")\n",
    "    img = gauss_filter(device, img)\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**滤波实例**\n",
    "\n",
    "| ![](images/gauss_girl.jpg) | ![](images/gauss_filter.jpg) |\n",
    "| -------------------------- | ---------------------------- |\n",
    "|原图 | Gaussian 3x3 Filter 10 times |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　选用不同的核，可以实现不同的滤波效果，如图像锐化，浮雕，运动模糊(相机向一个方向运动带来的噪声)等等，也可以实现简易的边缘检测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 导向滤波\n",
    "\n",
    "　　导向滤波是一种保边滤波，由何凯明博士提出，原理可以简单地用下图描述。\n",
    "![](images/guidedfilter.png)\n",
    "\n",
    "　　详细参见作者的论文，(http://kaiminghe.com/eccv10/) ，限于篇幅，此处从简。下面的实现是快速算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Guided filter with r, e\n",
    "    \"\"\"\n",
    "    def __init__(self, r, e):\n",
    "        super(GuidedFilter, self).__init__()\n",
    "        self.radius = r\n",
    "        self.eps = e\n",
    "\n",
    "    def box_sum(self, mat, r):\n",
    "        \"\"\"\n",
    "            # Ai = Si+r - Si-r-1\n",
    "            ===> i + r < n, i-r-1 >= 0\n",
    "            ===> [0, r + 1), [r + 1, n - r), [n - r, n]\n",
    "        \"\"\"\n",
    "        height, width = mat.size(0), mat.size(1)\n",
    "        assert 2 * r + 1 <= height\n",
    "        assert 2 * r + 1 <= width\n",
    "\n",
    "        dmat = torch.zeros_like(mat)\n",
    "\n",
    "        mat = torch.cumsum(mat, dim=0)\n",
    "        dmat[0:r + 1, :] = mat[r:2 * r + 1, :]\n",
    "        dmat[r + 1:height -\n",
    "             r, :] = mat[2 * r + 1:height, :] - mat[0:height - 2 * r - 1, :]\n",
    "        for i in range(height - r, height):\n",
    "            dmat[i, :] = mat[height - 1, :] - mat[i - r - 1, :]\n",
    "\n",
    "        dmat = torch.cumsum(dmat, dim=1)\n",
    "        mat[:, 0:r + 1] = dmat[:, r:2 * r + 1]\n",
    "        mat[:, r + 1:width -\n",
    "            r] = dmat[:, 2 * r + 1:width] - dmat[:, 0:width - 2 * r - 1]\n",
    "        for j in range(width - r, width):\n",
    "            mat[:, j] = dmat[:, width - 1] - dmat[:, j - r - 1]\n",
    "        return mat\n",
    "\n",
    "    def box_filter(self, x, N):\n",
    "        \"\"\"\n",
    "        x format is 1xCxHxW, here C = 3\n",
    "        \"\"\"\n",
    "        y = torch.zeros_like(x)\n",
    "        for i in range(x.size(1)):\n",
    "            y[0][i] = self.box_sum(x[0][i], self.radius).div(N)\n",
    "        return y\n",
    "\n",
    "    def forward(self, i, p):\n",
    "        N = torch.ones_like(i[0][0])\n",
    "        N = self.box_sum(N, self.radius)\n",
    "        mean_i = self.box_filter(i, N)\n",
    "        mean_p = self.box_filter(p, N)\n",
    "        mean_pi = self.box_filter(p * i, N)\n",
    "        mean_ii = self.box_filter(i * i, N)\n",
    "\n",
    "        cov_ip = mean_pi - mean_p * mean_i\n",
    "        cov_ii = mean_ii - mean_i * mean_i\n",
    "\n",
    "        a = cov_ip / (cov_ii + self.eps)\n",
    "        b = mean_p - a * mean_i\n",
    "\n",
    "        q = a * i + b\n",
    "        q.clamp_(0, 1)\n",
    "\n",
    "        return q\n",
    "\n",
    "    def self_guided(self, p):\n",
    "        N = torch.ones_like(p[0][0])\n",
    "        N = self.box_sum(N, self.radius)\n",
    "        mean_p = self.box_filter(p, N)\n",
    "        mean_pp = self.box_filter(p * p, N)\n",
    "\n",
    "        cov_pp = mean_pp - mean_p * mean_p\n",
    "\n",
    "        a = cov_pp / (cov_pp + self.eps)\n",
    "        b = mean_p - a * mean_p\n",
    "\n",
    "        q = a * p + b\n",
    "        q.clamp_(0, 1)\n",
    "\n",
    "        return q\n",
    "\n",
    "def guided_filter(device, i, p, r=3, e=0.01):\n",
    "    model = GuidedFilter(r, e)\n",
    "    model.to(device)\n",
    "    ti = to_tensor(i)\n",
    "    tp = to_tensor(p)\n",
    "    t = model(ti, tp)\n",
    "\n",
    "    return from_tensor(t)\n",
    "\n",
    "\n",
    "def self_guided_filter(device, img, r=5, e=0.01):\n",
    "    model = GuidedFilter(r, e)\n",
    "    model.to(device)\n",
    "\n",
    "    t = to_tensor(img)\n",
    "    t = model.self_guided(t)\n",
    "\n",
    "    return from_tensor(t)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    img = open(\"images/guided_girl.jpg\")\n",
    "    img = self_guided_filter(device, img, 10, 0.01)\n",
    "\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**美容实例**\n",
    "\n",
    "\n",
    "| ![](images/guided_girl.jpg) | ![](images/guided_filter.jpg)           |\n",
    "| --------------------------- | --------------------------------------- |\n",
    "|       原图                      | Guided Filter: Radius = 10,  eps = 0.01 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**增强实例**\n",
    "\n",
    "| ![](images/enhance.jpg) | ![](images/enhance_filter.jpg)         |\n",
    "| ----------------------- | -------------------------------------- |\n",
    "|      原图                | Guided Filter: Radius = 10, eps = 0.01 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.3 图像去雾\n",
    "\n",
    "　　下面的去雾算法，基于暗通道先验，同样由何凯明博士发明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DehazeFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Dehaze filter with r\n",
    "    \"\"\"\n",
    "    def __init__(self, r=7):\n",
    "        super(DehazeFilter, self).__init__()\n",
    "        self.radius = r\n",
    "        self.maxpool = nn.MaxPool2d(2 * r + 1, stride=1, padding=r)\n",
    "\n",
    "    def min_filter(self, x):\n",
    "        \"\"\"\n",
    "        suppose x is : HxW, y ==> 1x1xHxW\n",
    "        \"\"\"\n",
    "        y = x.unsqueeze(0).unsqueeze(0)\n",
    "        y = y * (-1.0)\n",
    "        y = self.maxpool(y)\n",
    "        y = y * (-1.0)\n",
    "        return y.squeeze(0).squeeze(0)\n",
    "\n",
    "    def dark_channel(self, x):\n",
    "        rgb = x[0]\n",
    "        dc, _ = torch.min(rgb, dim=0)\n",
    "        # dc size: HxW\n",
    "        dc = self.min_filter(dc)\n",
    "        return dc\n",
    "\n",
    "    def atmos_light(self, dc, x):\n",
    "        # dc -- HxW\n",
    "        sorted, _ = dc.view(-1).sort(descending=True)\n",
    "        index = int(dc.size(0) * dc.size(1) / 1000)\n",
    "        thres = sorted[index].item()\n",
    "        mask = dc.ge(thres)\n",
    "\n",
    "        a = torch.zeros(3)\n",
    "        for i in range(3):\n",
    "            rgb = x[0][i]\n",
    "            dx = torch.masked_select(rgb, mask)\n",
    "            a[i] = torch.mean(dx)\n",
    "\n",
    "        # RGB atmos light\n",
    "        avg = 0.299 * a[0].item() + 0.587 * a[1].item() + 0.114 * a[2].item()\n",
    "        a[0] = a[1] = a[2] = avg\n",
    "\n",
    "        return a[0].item(), a[1].item(), a[2].item()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        I = J*t + A*(1-t), here I = x, target is J\n",
    "        t = 1.0 - omega*min_filter(Ic/Ac) for c = R, G, B, here w = 0.95\n",
    "        J = (Ic - Ac)/t + Ac\n",
    "        \"\"\"\n",
    "        omega = 0.95\n",
    "\n",
    "        dc = self.dark_channel(x)\n",
    "\n",
    "        # atmos light\n",
    "        a_r, a_g, a_b = self.atmos_light(dc, x)\n",
    "\n",
    "        # t -- from 1x3xHxW--> HxW\n",
    "        t = torch.zeros_like(x)\n",
    "        t[0][0] = x[0][0] / a_r\n",
    "        t[0][1] = x[0][1] / a_g\n",
    "        t[0][2] = x[0][2] / a_b\n",
    "        t = self.dark_channel(t)\n",
    "        t = 1 - omega * t\n",
    "\n",
    "        refined_t = torch.zeros_like(x)\n",
    "        refined_t[0][0] = t\n",
    "        refined_t[0][1] = t\n",
    "        refined_t[0][2] = t\n",
    "\n",
    "        model = GuidedFilter(60, 0.0001)\n",
    "        model.to(device)\n",
    "        refined_t = model(x, refined_t)\n",
    "\n",
    "        refined_t.clamp_(min=0.1)\n",
    "\n",
    "        y = torch.zeros_like(x)\n",
    "        y[0][0] = (x[0][0] - a_r) / refined_t[0][0] + a_r\n",
    "        y[0][1] = (x[0][1] - a_g) / refined_t[0][1] + a_g\n",
    "        y[0][2] = (x[0][2] - a_b) / refined_t[0][2] + a_b\n",
    "        y.clamp_(0, 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "def dehaze_filter(device, img, r=3):\n",
    "    model = DehazeFilter(r)\n",
    "    model.to(device)\n",
    "    t = to_tensor(img)\n",
    "    t = model(t)\n",
    "    return from_tensor(t)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    img = open(\"images/haze.jpg\")\n",
    "    img = dehaze_filter(device, img, 5)\n",
    "    img.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**去雾实例**\n",
    "\n",
    "| ![](images/haze.jpg) | ![](images/haze_filter.jpg) |\n",
    "| -------------------- | --------------------------- |\n",
    "|    原图                  | Haze Filter: Radius = 5     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1.4.4 完整资源\n",
    "\n",
    "https://github.com/delldu/BeyondFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 自动求导\n",
    "　　计算图是一种有向无环图(**DAG**)，用于记录算子与变量间的关系，它是现代深度学习框架的核心，为高效自动求导算法—反向传播(Back Propogation)提供支持。从理论上讲，有了计算图，要计算各节点的梯度，只需从根节点出发，自动沿计算图反向传播，就能计算出每个叶节点的梯度。但是，手动实现反向传播，费时费力，容易出错。为此，PyTorch专门开发出自动求导引擎torch.autograd。在前向传播中，autograd会记录当前Tensor的所有操作，并建立计算图。在反向传播中，autograd会沿着这个图从根节点回溯，利用链式求导法则计算叶节点的梯度。每个前向传播操作的函数都有对应的反向传播函数，用来计算节点的梯度，这些函数名通常以Backward结尾。\n",
    "\n",
    "　　**实例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "def test_autograd():\n",
    "    x = torch.ones(3, 3, requires_grad=True)\n",
    "    y = x.sum()\n",
    "\n",
    "    y.backward()\n",
    "    print(x.grad)\n",
    "\n",
    "    # 梯度清零\n",
    "    x.grad.data.zero_()\n",
    "\n",
    "test_autograd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　**注意：**\n",
    "  \n",
    "　　1. 梯度在反向传播过程中是累加的，即每次运行反向传播，梯度都会累加之前的梯度，所以反向传播前应把梯度清零;\n",
    "  \n",
    "　　2. 具备requires_grad的Tensor要支持in-place操作十分困难，因为这些函数会修改Tensor自身，而在反向传播中，Tensor需要缓存原来的数据来计算梯度，所以在计算中应尽力避免使用这些函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 向量化思维\n",
    "　　向量化计算是一种特殊的并行计算，相对于一般程序同时只执行一个操作的方式，它可同时执行多个操作，通常是对不同数据执行同样的一个或一批指令，或者说把指令应用于一个向量上。\n",
    "\n",
    "　　Python是一门高级语言，使用方便，但很多操作低效，尤其是for循环，所以应尽量调用内建函数(buildin-function)，因为这些函数底层由C/C++实现，经过底层优化。我们在平常开发代码时，应养成**向量化的思维习惯**，避免对较大的Tensor逐元遍历，对PyTorch没有的功能，如果性能非常关键，应通过C/C++扩展完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
